{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## script: hashing_base.ipynb \n",
    "## author: california policy lab \n",
    "## purpose: this is a general-purpose hashing script that will hash the \n",
    "##          following identifying variables (and various permutations of those variables)\n",
    "##          in the source data. This script should be modified for each dataset it is run on.\n",
    "##          1. SSN\n",
    "##          2. DOB\n",
    "##             - Day\n",
    "##             - Month\n",
    "##             - Year\n",
    "##          3. Name\n",
    "##             - First\n",
    "##             - Last\n",
    "##      Some commonly needed modifications include:\n",
    "##      1. Changing the salt to whatever is applicable for the project\n",
    "##      2. Changing the latest year to the year corresponding to the latest year of data \n",
    "##      3. Changing the variable names to correspond to the raw data \n",
    "##      4. Modifying as necessary to hash additional PII, or PII across multiple files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import hashlib\n",
    "from abydos.phonetic import Soundex\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show more columns\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# Turn on debugger for errors\n",
    "# %pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "special_characters = pd.read_csv(\n",
    "    'Unicode Values Sheet - unicode_short (1).csv',\n",
    "    dtype=str\n",
    ")\n",
    "# Clean up the column names a little\n",
    "special_characters = special_characters.rename(columns={x: x.strip() for x in special_characters.columns})\n",
    "special_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "special_characters_dict = special_characters.set_index('Symbol')['To Value'].to_dict()\n",
    "special_characters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_preprocess_data(\n",
    "    input_filepath,\n",
    "    ssn,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    dob,\n",
    "    latest_year,\n",
    "    date_format=None,\n",
    "    coerce_date=False,\n",
    "    **kwargs\n",
    "):\n",
    "    \n",
    "    # import raw data, using strings for all columns except DOB, which is a date\n",
    "    raw = pd.read_csv(\n",
    "        input_filepath,\n",
    "        dtype=str,\n",
    "        parse_dates=[dob] if dob and not date_format else [],\n",
    "        # Don't want anything to be read as NaN so that the substrings etc will be well defined;\n",
    "        # we will replace all empty strings with NaN as we gp\n",
    "        keep_default_na=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    print('CSV imported')\n",
    "    \n",
    "    # Read in the table of special characters we want to convert\n",
    "    special_characters = pd.read_csv(\n",
    "        'Unicode Values Sheet - unicode_short (1).csv',\n",
    "        dtype=str\n",
    "    )\n",
    "    # Clean up the column names a little\n",
    "    special_characters = special_characters.rename(columns={x: x.strip() for x in special_characters.columns})\n",
    "    # Convert to a dictionary\n",
    "    special_characters_dict = special_characters.set_index('Symbol')['To Value'].to_dict()\n",
    "\n",
    "    if date_format:\n",
    "        # Need to parse datetimes after reading in the CSV if it's a custom format; this is the\n",
    "        # the way pandas recommends you do it in their read_csv documentation\n",
    "        raw[dob] = pd.to_datetime(raw[dob], format=date_format, errors='coerce' if coerce_date else 'raise')\n",
    "\n",
    "    if ssn:\n",
    "        ## clean SSNs\n",
    "        ## remove all non-digit characters \n",
    "        raw['ssn'] = raw[ssn].str.replace(r'[^0-9]', '', regex=True)\n",
    "        # Pad SSNs to 9 characters\n",
    "        raw['ssn'] = raw['ssn'].str.pad(width=9, side='left', fillchar='0')\n",
    "        ## tag invalid SSNs    \n",
    "        raw['badSSN'] = (\n",
    "            (raw['ssn'] == '') |\n",
    "            (raw['ssn'].str.len() > 9) |\n",
    "            (raw['ssn'].str[0:3].isin([\"000\", \"666\"])) |\n",
    "            (raw['ssn'].str[3:5]==\"00\") |\n",
    "            (raw['ssn'].str[5:9]==\"0000\") |\n",
    "            (raw['ssn'].str.startswith('9')) |\n",
    "            (raw['ssn'].isin([\"078051120\", \"123456789\"]))\n",
    "        )\n",
    "        # Make SSN missing where badSSN is true\n",
    "        raw['ssn'] = raw['ssn'].mask(raw['badSSN'])\n",
    "\n",
    "        ## create ssn substrings to hash\n",
    "        # Do this with a loop over the first of the two digits to hold out\n",
    "        for i in range(1, 9):\n",
    "            # ie `raw['ssn56'] = raw['ssn'].str[0:4] + raw['ssn'].str[6:9]\n",
    "            # Only want to do this where badSSN is not true\n",
    "            # This implicitly sets NaN where the condition in loc isn't met\n",
    "            raw.loc[~raw['badSSN'], 'ssn' + str(i) + str(i+1)] = raw['ssn'].str[0:i-1] + raw['ssn'].str[i+1:9]\n",
    "\n",
    "        \n",
    "    ## clean names\n",
    "\n",
    "\n",
    "    pe = Soundex()\n",
    "    if first_name:\n",
    "        ## remove common prefixes/suffixes from first & last name, making sure to escape the periods\n",
    "        # so we can use them in a regex\n",
    "        fn_prefix_titles = ['mr', 'mrs', 'dr', 'ms']\n",
    "        fn_prefix_formats = ['{}\\\\ ', '{}\\\\.', '{}$']\n",
    "        # Take cartesian product of the title and possible formats for them and make them a single \"or\" regex\n",
    "        fn_prefixes = '|'.join([prefix_format.format(title) for prefix_format in\n",
    "                       fn_prefix_formats for title in fn_prefix_titles])\n",
    "        raw['fn'] = (raw[first_name]\n",
    "                     # str.maketrans makes a translation table from a dictionary; it converts the special characters\n",
    "                     # to numeric codes, which it can then evidently parse (it doesn't work without the call to maketrans)\n",
    "                     .str.translate(str.maketrans(special_characters_dict))  # Convert to 26-character alphabet characters\n",
    "                     .str.lower()  # Make lowercase\n",
    "                     .str.strip()  # Strip whitespace at ends of string\n",
    "                     # Replace prefixes at beginning of string\n",
    "                     .str.replace(r'^({})+'.format(fn_prefixes), '', regex=True)\n",
    "                     # Remove all non-alphabetical characters\n",
    "                     .str.replace(r'[^a-z]', '', regex=True))\n",
    "        # ie the regex this gives will be '^(mr|mrs|dr\\.|...|ms\\.)', meaning match any of those at the start\n",
    "        # of the string, and analogously at the end of the string for ln\n",
    "        # Fill both with nan where empty\n",
    "        raw['fn'] = raw['fn'].mask(raw['fn'] == '')\n",
    "        # Tag bad names\n",
    "        raw['badFN'] = raw['fn'].isnull()\n",
    "        ## create shortened and soundex vesions of names, for non bad names\n",
    "        raw.loc[~raw['badFN'], 'fn2l'] = raw.loc[~raw['badFN'], 'fn'].str[0:2]\n",
    "        raw.loc[~raw['badFN'], 'fn4l'] = raw.loc[~raw['badFN'], 'fn'].str[0:4]\n",
    "        # Add 1l in here temporarily for comparison to old methodology\n",
    "        # TODO: delete me\n",
    "        raw.loc[~raw['badFN'], 'fn1l'] = raw.loc[~raw['badFN'], 'fn'].str[0:1]\n",
    "        raw.loc[~raw['badFN'], 'fn_sdx'] = raw.loc[~raw['badFN'], 'fn'].apply(pe.encode)\n",
    "\n",
    "    if last_name:\n",
    "        ln_suffixes = '|'.join([re.escape(x) for x in [' jr',' sr', ' jr.', ' sr.']])\n",
    "        raw['ln'] = (raw[last_name]\n",
    "                     # str.maketrans makes a translation table from a dictionary; it converts the special characters\n",
    "                     # to numeric codes, which it can then evidently parse (it doesn't work without the call to maketrans)\n",
    "                     .str.translate(str.maketrans(special_characters_dict))  # Convert to 26-character alphabet characters\n",
    "                     .str.lower()\n",
    "                     .str.strip()\n",
    "                     # Replace suffixes at end of string\n",
    "                     .str.replace(r'({})+$'.format(ln_suffixes), '', regex=True)\n",
    "                     .str.replace(r'[^a-z]', '', regex=True))\n",
    "        raw['ln'] = raw['ln'].mask(raw['ln'] == '')\n",
    "        raw['badLN'] = raw['ln'].isnull()\n",
    "        raw.loc[~raw['badLN'], 'ln2l'] = raw.loc[~raw['badLN'], 'ln'].str[0:2]\n",
    "        raw.loc[~raw['badLN'], 'ln4l'] = raw.loc[~raw['badLN'], 'ln'].str[0:4]\n",
    "        raw.loc[~raw['badLN'], 'ln1l'] = raw.loc[~raw['badLN'], 'ln'].str[0:1]\n",
    "        raw.loc[~raw['badLN'], 'ln_sdx'] = raw.loc[~raw['badLN'], 'ln'].apply(pe.encode)\n",
    "\n",
    "## clean DOB\n",
    "    ## extract day, month & year\n",
    "    # Missing values are populated with pd.NaT (Not a Time, pandas' representation of missing dates)\n",
    "    # when parsing the csv, even if keep_default_na is set to False (which makes sense, not sure what\n",
    "    # else they could be)\n",
    "    # NaT.year gives NaN, so this propagates missing values appropriately\n",
    "    if dob:\n",
    "        raw['dob_y'] = raw[dob].dt.year\n",
    "        raw['dob_m'] = raw[dob].dt.month\n",
    "        raw['dob_d'] = raw[dob].dt.day\n",
    "        ## tag invalid dates\n",
    "        raw['badDOB'] = (\n",
    "            (pd.isnull(raw[dob])) |\n",
    "            (raw['dob_y'] < 1900) |\n",
    "            (raw['dob_y'] > latest_year)\n",
    "        )\n",
    "        ## convert to string for hashing to work (where we haven't flagged it as bad)\n",
    "        raw.loc[~raw['badDOB'], 'dob_y'] = raw.loc[~raw['badDOB'], 'dob_y'].apply(lambda x: '{:.0f}'.format(x))\n",
    "        raw.loc[~raw['badDOB'], 'dob_m'] = raw.loc[~raw['badDOB'], 'dob_m'].apply(lambda x: '{:.0f}'.format(x))\n",
    "        raw.loc[~raw['badDOB'], 'dob_d'] = raw.loc[~raw['badDOB'], 'dob_d'].apply(lambda x: '{:.0f}'.format(x))\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "## main function to import, clean, and hash data \n",
    "def hash_data(salt, \n",
    "              input_filepath, \n",
    "              output_filepath,  \n",
    "              ssn, \n",
    "              first_name, \n",
    "              last_name, \n",
    "              dob,\n",
    "              latest_year,\n",
    "              drop_unhashed_pii=True,\n",
    "              coerce_date=False,\n",
    "             **kwargs):\n",
    "    \"\"\" Hash PII (SSN, first name, last name, and date of birthday) and derivatives\n",
    "        from an input CSV file. \n",
    "        \n",
    "        Parameters \n",
    "        ----------\n",
    "        \n",
    "        salt: string \n",
    "            the agreed-upon salt  \n",
    "        input_filepath: string \n",
    "            the local filepath of the input csv \n",
    "        output_filepath: string\n",
    "            the local filepath of the output csv \n",
    "        ssn: string\n",
    "            the name of the ssn variable in the input csv \n",
    "        first_name: string \n",
    "            the name of the first name variable in the input csv \n",
    "        last_name: string \n",
    "            the name of the last name variable in the input csv \n",
    "        dob: string\n",
    "            the name of the birthdate variable in the input csv\n",
    "        latest_year: integer \n",
    "            the latest plausible birthyear in the input data\n",
    "        kwargs: dict\n",
    "            Keyword arguments that are passed to clean_and_preprocess_data\n",
    "       \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        hashed data: a csv file with all programmatic data plus hashed PII \n",
    "    \"\"\"\n",
    "\n",
    "    raw = clean_and_preprocess_data(\n",
    "        input_filepath=input_filepath,\n",
    "        ssn=ssn,\n",
    "        first_name=first_name,\n",
    "        last_name=last_name,\n",
    "        dob=dob,\n",
    "        latest_year=latest_year,\n",
    "        coerce_date=coerce_date,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    ## drop intermediate & raw vars, do some renaming    \n",
    "    vars_to_hash = (\n",
    "        (['ssn','ssn89','ssn78','ssn67','ssn56','ssn45',\n",
    "         'ssn34','ssn23','ssn12'] if ssn else []) +\n",
    "        (['fn', 'fn2l', 'fn1l', 'fn4l','fn_sdx'] if first_name else []) +\n",
    "        (['ln', 'ln2l', 'ln1l', 'ln4l','ln_sdx'] if last_name else []) +\n",
    "        (['dob_d', 'dob_m', 'dob_y'] if dob else [])\n",
    ")\n",
    "    clean = raw[vars_to_hash + [x for x in raw.columns if x not in vars_to_hash and \n",
    "                                x not in ([ssn, first_name, last_name, dob] if drop_unhashed_pii else [])]]\n",
    "    \n",
    "\n",
    "## hash SSN and derivatives, first and last name and derivatives, and DOB and derivatives\n",
    "#     pdb.set_trace()\n",
    "    for col in vars_to_hash:\n",
    "#         print(col)\n",
    "        # Get which base field this is a part of, so we can not hash fields where it's flagged as bad\n",
    "        this_base_col = re.search(r'^([a-z]+)', col).group(1)\n",
    "        this_bad_flag = 'bad' + this_base_col.upper()\n",
    "        clean.loc[~clean[this_bad_flag], col + '_hash'] = clean.loc[~clean[this_bad_flag], col].apply(\n",
    "            lambda s: hashlib.sha256(str.encode(salt+s)).hexdigest().upper()\n",
    "        )\n",
    "\n",
    "## drop unhashed PII and export\n",
    "    if drop_unhashed_pii:\n",
    "        clean = clean.drop(columns=vars_to_hash)\n",
    "    clean.to_csv(output_filepath, sep=\",\", index=False)  \n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashed = hash_data(\n",
    "    salt=\"FAKE2!59U~9668SALT\",\n",
    "    input_filepath=\"\", \n",
    "    output_filepath=\"\", \n",
    "    ssn=None, \n",
    "    first_name=None, \n",
    "    last_name=None, \n",
    "    dob='',\n",
    "    latest_year=2021,\n",
    "    sep=',',\n",
    "    drop_unhashed_pii=False,\n",
    "    coerce_date=True,\n",
    "    date_format='%Y-%m-%d'\n",
    ")\n",
    "hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashed = hash_data(\n",
    "    salt=\"FAKE2!59U~9668SALT\",\n",
    "    input_filepath=\"\", \n",
    "    output_filepath=\"\", \n",
    "    ssn='', \n",
    "    first_name=None, \n",
    "    last_name=None, \n",
    "    dob=None,\n",
    "    latest_year=2021,\n",
    "    sep='|',\n",
    "    drop_unhashed_pii=False\n",
    ")\n",
    "hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FN/LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "hashed = hash_data(\n",
    "    salt=\"FAKE2!59U~9668SALT\",\n",
    "    input_filepath=\"\", \n",
    "    output_filepath=\"\", \n",
    "    ssn=None, \n",
    "    first_name='', \n",
    "    last_name='',\n",
    "    dob=None,\n",
    "    latest_year=2021,\n",
    "    sep='|',\n",
    "    nrows=None,\n",
    "    drop_unhashed_pii=False,\n",
    "    error_bad_lines=False,\n",
    "    warn_bad_lines=True,\n",
    "    quoting=csv.QUOTE_NONE\n",
    ")\n",
    "hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
